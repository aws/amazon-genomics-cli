<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amazon Genomics CLI â€“ Workflow Engines</title>
    <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/</link>
    <description>Recent content in Workflow Engines on Amazon Genomics CLI</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 01 Oct 2021 17:23:58 -0400</lastBuildDate>
    
	  <atom:link href="https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Filesystems</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/filesystems/</link>
      <pubDate>Fri, 11 Mar 2022 14:31:11 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/filesystems/</guid>
      <description>
        
        
        

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Attention&lt;/h4&gt;

    &lt;strong&gt;The Amazon Genomics CLI project has entered its End Of Life (EOL) phase&lt;/strong&gt;. The code is no longer actively maintained and the &lt;strong&gt;Github repository will be archived on May 31 2024&lt;/strong&gt;. During this time, we encourage customers to migrate to &lt;a href=&#34;https://aws.amazon.com/healthomics/&#34;&gt;AWS HealthOmics&lt;/a&gt; to run their genomics workflows on AWS, or &lt;a href=&#34;https://aws.amazon.com/contact-us/?nc2=h_header&#34;&gt;reach out to their AWS account team&lt;/a&gt; for alternative solutions. While the source code of AGC will still be available after the EOL date, we will not make any updates inclusive of addressing issues or accepting Pull Requests.

&lt;/div&gt;

&lt;p&gt;The tasks in a workflow require a common filesystem or scratch space where the outputs of tasks can be written so they
are available to the inputs of dependent tasks in the same workflow. The following pages provide details on the engine
filesystems that can be deployed by Amazon Genomics CLI.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: miniwdl</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/miniwdl/</link>
      <pubDate>Fri, 01 Oct 2021 17:27:31 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/miniwdl/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://miniwdl.readthedocs.io/en/latest/index.html&#34;&gt;miniwdl&lt;/a&gt; is free open source software distributed under the MIT licence
developed by the &lt;a href=&#34;https://chanzuckerberg.com/&#34;&gt;Chan Zuckerberg Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The source code for miniwdl is available on &lt;a href=&#34;https://github.com/chanzuckerberg/miniwdl&#34;&gt;GitHub&lt;/a&gt;. When deployed with
Amazon Genomics CLI miniwdl makes use of the &lt;a href=&#34;https://github.com/miniwdl-ext/miniwdl-aws&#34;&gt;miniwdl-aws extension&lt;/a&gt; which is
also distributed under the MIT licence.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;There are four components of a miniwdl engine as deployed in an Amazon Genomics CLI context:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MiniWDLContextArch.png&#34; alt=&#34;Image of infrastructure deployed in a miniwdl context&#34; title=&#34;miniWDL Context Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;wes-adapter&#34;&gt;WES Adapter&lt;/h3&gt;
&lt;p&gt;Amazon Genomics CLI communicates with the miniwdl engine via a GA4GH &lt;a href=&#34;https://github.com/ga4gh/workflow-execution-service-schemas&#34;&gt;WES&lt;/a&gt; REST service. The WES Adapter implements
the WES standard and translates WES calls into calls to the miniwdl head process.&lt;/p&gt;
&lt;h3 id=&#34;head-compute-environment&#34;&gt;Head Compute Environment&lt;/h3&gt;
&lt;p&gt;For every workflow submitted, the WES adapter will create a new AWS Batch Job that contains the miniwdl process responsible
for running that workflow. These miniwdl &amp;ldquo;head&amp;rdquo; jobs are run in an &amp;ldquo;On-demand&amp;rdquo; AWS Fargate compute environment even when the actual workflow
tasks run in a Spot environment. This is to prevent Spot interruptions from terminating the workflow coordinator.&lt;/p&gt;
&lt;h3 id=&#34;task-compute-environment&#34;&gt;Task Compute Environment&lt;/h3&gt;
&lt;p&gt;Workflow tasks are submitted by the miniwdl head job to an AWS Batch queue and run in containers using an AWS Compute Environment.
Container characteristics are defined by the resources requested in the workflow configuration. AWS Batch coordinates the elastic provisioning of EC2 instances (container hosts)
based on the available work in the queue. Batch will place containers on container hosts as space allows.&lt;/p&gt;
&lt;h4 id=&#34;session-cache-and-input-localization&#34;&gt;Session Cache and Input Localization&lt;/h4&gt;
&lt;p&gt;Any context with a miniwdl engine will use an Amazon Elastic File System (EFS) volume as scratch space. Inputs from S3 are
localized to the volume by jobs that the miniwdl engine spawns to copy these files to the volume. Outputs are copied back
to S3 using a similar process. Workflow tasks access the EFS volume to obtain inputs and write intermediates and outputs.&lt;/p&gt;
&lt;p&gt;The EFS volume is used by all miniwdl engine &amp;ldquo;head&amp;rdquo; jobs to store metadata necessary for call caching.&lt;/p&gt;
&lt;p&gt;The EFS volume will remain in your account for the lifetime of the context and are destroyed when contexts are destroyed.
Because the volume will grow in size as you run more workflows we recommend destroying the context when done to avoid on going EFS
charges.&lt;/p&gt;
&lt;h2 id=&#34;using-miniwdl-as-a-context-engine&#34;&gt;Using miniwdl as a Context Engine&lt;/h2&gt;
&lt;p&gt;You may declare miniwdl to be the &lt;code&gt;engine&lt;/code&gt; for any contexts &lt;code&gt;wdl&lt;/code&gt; type engine. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;contexts&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;onDemandCtx&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;requestSpotInstances&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;engines&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;      &lt;/span&gt;- &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;wdl&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;engine&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;miniwdl&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;call-caching&#34;&gt;Call Caching&lt;/h2&gt;
&lt;p&gt;Call caching is enabled by default for miniwdl and because the metadata is stored in the contexts EFS volume call caching
will work across different engine &amp;ldquo;head&amp;rdquo; jobs.&lt;/p&gt;
&lt;p&gt;To disable call caching you can provide the &lt;code&gt;--no-cache&lt;/code&gt; engine option. You may do this in a workflows &lt;code&gt;MANIFEST.json&lt;/code&gt; by
adding the following key/ value pair.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  &amp;quot;engineOptions&amp;quot;: &amp;quot;--no-cache&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Toil</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/toil/</link>
      <pubDate>Tue, 26 Apr 2022 15:34:00 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/toil/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://toil.ucsc-cgl.org/&#34;&gt;Toil&lt;/a&gt; is a workflow engine developed by the
&lt;a href=&#34;https://cglgenomics.ucsc.edu/&#34;&gt;Computational Genomics Lab&lt;/a&gt; at the
&lt;a href=&#34;https://genomics.ucsc.edu/&#34;&gt;UC Santa Cruz Genomics Institute&lt;/a&gt;. In Amazon Genomics
CLI, Toil is an engine that can be deployed in a
&lt;a href=&#34;https://aws.github.io/amazon-genomics-cli/amazon-genomics-cli/docs/concepts/contexts/&#34;&gt;context&lt;/a&gt; as an
&lt;a href=&#34;https://aws.github.io/amazon-genomics-cli/amazon-genomics-cli/docs/concepts/engines/&#34;&gt;engine&lt;/a&gt; to run workflows written in the
&lt;a href=&#34;https://www.commonwl.org/&#34;&gt;Common Workflow Language&lt;/a&gt; (CWL) standard, version
&lt;a href=&#34;https://www.commonwl.org/v1.0/&#34;&gt;v1.0&lt;/a&gt;, &lt;a href=&#34;https://www.commonwl.org/v1.1/&#34;&gt;v1.1&lt;/a&gt;,
and &lt;a href=&#34;https://www.commonwl.org/v1.2/&#34;&gt;v1.2&lt;/a&gt; (or mixed versions).&lt;/p&gt;
&lt;p&gt;Toil is an open source project distributed by UC Santa Cruz under the &lt;a href=&#34;https://github.com/DataBiosphere/toil/blob/master/LICENSE&#34;&gt;Apache 2
license&lt;/a&gt; and
available on
&lt;a href=&#34;https://github.com/DataBiosphere/toil&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;There are two components of a Toil engine as deployed in an Amazon Genomics
CLI context:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ToilContextArch.png&#34; alt=&#34;Image of infrastructure deployed in a Toil context&#34; title=&#34;Toil Context Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;toil-server&#34;&gt;Toil Server&lt;/h3&gt;
&lt;p&gt;The Toil engine is run in &amp;ldquo;server mode&amp;rdquo; as a container service in ECS. The
engine can run multiple workflows asynchronously. Workflow tasks are run in an
elastic &lt;a href=&#34;#compute-environment&#34;&gt;compute environment&lt;/a&gt; and monitored by Toil.
Amazon Genomics CLI communicates with the Toil engine via a GA4GH
&lt;a href=&#34;https://github.com/ga4gh/workflow-execution-service-schemas&#34;&gt;WES&lt;/a&gt; REST service
which the server offers, available via API Gateway.&lt;/p&gt;
&lt;h3 id=&#34;task-compute-environment&#34;&gt;Task Compute Environment&lt;/h3&gt;
&lt;p&gt;Workflow tasks are submitted by Toil to an AWS Batch queue and run in
Toil-provided containers using an AWS Compute Environment. Tasks which use the
&lt;a href=&#34;https://www.commonwl.org/user_guide/07-containers/index.html&#34;&gt;CWL &lt;code&gt;DockerRequirement&lt;/code&gt;&lt;/a&gt;
will additionally be run in sibling containers on the host Docker daemon. AWS
Batch coordinates the elastic provisioning of EC2 instances (container hosts)
based on the available work in the queue. Batch will place containers on
container hosts as space allows.&lt;/p&gt;
&lt;h4 id=&#34;disk-expansion&#34;&gt;Disk Expansion&lt;/h4&gt;
&lt;p&gt;Container hosts in the Batch compute environment use EBS volumes as local
scratch space. As an EBS volume approaches a capacity threshold, new EBS
volumes will be attached and merged into the file system. These volumes are
destroyed when AWS Batch terminates the container host. CWL disk space
requirements are ignored by Toil when running against AWS Batch.&lt;/p&gt;
&lt;p&gt;This setup means that workflows that succeed on AGC may fail on other CWL
runners (because they do not request enough disk space) and workflows that
succeed on other CWL runners may fail on AGC (because they allocate disk space
faster than the expansion process can react).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cromwell</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/cromwell/</link>
      <pubDate>Fri, 01 Oct 2021 17:27:21 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/cromwell/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cromwell.readthedocs.io/en/stable/&#34;&gt;Cromwell&lt;/a&gt; is a workflow engine developed by the &lt;a href=&#34;https://www.broadinstitute.org/&#34;&gt;Broad Institute&lt;/a&gt;.
In Amazon Genomics CLI, Cromwell is an engine that can be
deployed in a &lt;a href=&#34;https://aws.github.io/amazon-genomics-cli/amazon-genomics-cli/docs/concepts/contexts/&#34;&gt;context&lt;/a&gt; as an &lt;a href=&#34;https://aws.github.io/amazon-genomics-cli/amazon-genomics-cli/docs/concepts/engines/&#34;&gt;engine&lt;/a&gt;
to run workflows based on the &lt;a href=&#34;https://openwdl.org/&#34;&gt;WDL&lt;/a&gt; specification.&lt;/p&gt;
&lt;p&gt;Cromwell is an open source project distributed by the Broad Institute under the &lt;a href=&#34;https://github.com/broadinstitute/cromwell/blob/develop/LICENSE-ASL-2.0&#34;&gt;Apache 2 license&lt;/a&gt; and available on &lt;a href=&#34;https://github.com/broadinstitute/cromwell&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;customizations&#34;&gt;Customizations&lt;/h3&gt;
&lt;p&gt;Some minor customizations where made to the AWS Backend adapter for Cromwell to facilitate improved scalability and cross
region S3 bucket access when deployed with Amazon Genomics CLI. The fork containing these customizations is available &lt;a href=&#34;https://github.com/markjschreiber/cromwell&#34;&gt;here&lt;/a&gt;
and we are working to contribute these back to the main code base.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;There are four components of a Cromwell engine as deployed in an Amazon Genomics CLI context.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;CromwellContextArch.png&#34; alt=&#34;Image of infrastructure deployed in a Cromwell context&#34; title=&#34;Cromwell Context Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;wes-adapter&#34;&gt;WES Adapter&lt;/h3&gt;
&lt;p&gt;Amazon Genomics CLI communicates with the Cromwell engine via a GA4GH &lt;a href=&#34;https://github.com/ga4gh/workflow-execution-service-schemas&#34;&gt;WES&lt;/a&gt; REST service. The WES Adapter implements
the WES standard and translates WES calls into calls to the &lt;a href=&#34;https://cromwell.readthedocs.io/en/stable/api/RESTAPI/&#34;&gt;Cromwell REST API&lt;/a&gt;. The adapter runs as an Amazon ECS service
available via API Gateway.&lt;/p&gt;
&lt;h3 id=&#34;cromwell-server&#34;&gt;Cromwell Server&lt;/h3&gt;
&lt;p&gt;The Cromwell engine is run in &amp;ldquo;server mode&amp;rdquo; as a container service in ECS and receives instructions from the WES Adapter. The
engine can run multiple workflows asynchronously. Workflow tasks are run in an elastic &lt;a href=&#34;#compute-environment&#34;&gt;compute environment&lt;/a&gt; and
monitored by Cromwell.&lt;/p&gt;
&lt;h3 id=&#34;session-cache&#34;&gt;Session Cache&lt;/h3&gt;
&lt;p&gt;Cromwell can use workflow run metadata to perform call caching. When deployed by Amazon Genomics CLI call caching is enabled
by default. Metadata is stored by an embedded HSQL DB with file storage in an attached EFS volume. The EFS volume
exists for the lifetime of the context the engine is deployed in so re-runs of workflows within the lifetime can benefit
from call caching.&lt;/p&gt;
&lt;h3 id=&#34;task-compute-environment&#34;&gt;Task Compute Environment&lt;/h3&gt;
&lt;p&gt;Workflow tasks are submitted by Cromwell to an AWS Batch queue and run in containers using an AWS Compute Environment.
Container characteristics are defined by the &lt;code&gt;runtime&lt;/code&gt;. AWS Batch coordinates the elastic provisioning of EC2 instances (container hosts)
based on the available work in the queue. Batch will place containers on container hosts as space allows.&lt;/p&gt;
&lt;h4 id=&#34;fetch-and-run-strategy&#34;&gt;Fetch and Run Strategy&lt;/h4&gt;
&lt;p&gt;Execution of workflow tasks uses a &amp;ldquo;Fetch and Run&amp;rdquo; strategy. The commands specified in the &lt;code&gt;command&lt;/code&gt; section of the WDL task
are written as a file to S3 and &amp;ldquo;fetched&amp;rdquo; into the container and run.
The script is &amp;ldquo;decorated&amp;rdquo; with instructions to fetch any &lt;code&gt;File&lt;/code&gt; inputs from S3 and to write any &lt;code&gt;File&lt;/code&gt; outputs back to S3.&lt;/p&gt;
&lt;h4 id=&#34;disk-expansion&#34;&gt;Disk Expansion&lt;/h4&gt;
&lt;p&gt;Container hosts in the Batch compute environment use EBS volumes as local scratch space. As an EBS volume approaches a
capacity threshold, new EBS volumes will be attached and merged into the file system. These volumes are destroyed when
AWS Batch terminates the container host. For this reason it is not necessary to specify disk requirements for the task
&lt;code&gt;runtime&lt;/code&gt; and these WDL directives will be ignored.&lt;/p&gt;
&lt;h4 id=&#34;aws-batch-retries&#34;&gt;AWS Batch Retries&lt;/h4&gt;
&lt;p&gt;The Cromwell AWS Batch backend supports AWS Batch&amp;rsquo;s task &lt;a href=&#34;https://docs.aws.amazon.com/batch/latest/APIReference/API_RetryStrategy.html&#34;&gt;retry&lt;/a&gt; option allowing failed tasks to attempt to run again. This
can be useful for adding resilience to a workflow from sporadic infrastructure failures. It is especially useful when using
an Amazon Genomics CLI &amp;ldquo;spot&amp;rdquo; context as spot instances can be terminated with minimal warning. To enable retries, add
the following option to your &lt;code&gt;runtime&lt;/code&gt; section of a task:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;runtime {
    ...
    awsBatchRetryAttempts: &amp;lt;int&amp;gt;
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where &lt;code&gt;&amp;lt;int&amp;gt;&lt;/code&gt; is an integer specifying the number of retries up to a maximum of &lt;code&gt;10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although similar to the WDL &lt;code&gt;preemptible&lt;/code&gt; option, &lt;code&gt;awsBatchRetryAttempts&lt;/code&gt; has differences in how retries are implemented. Notably,
the implementation falls back on the AWS Batch retry strategy and will retry a task that fails for &lt;strong&gt;any&lt;/strong&gt; reason; whereas the &lt;code&gt;preemptible&lt;/code&gt;
option is more specific to failures caused by preemption. At this time the &lt;code&gt;preemptible&lt;/code&gt; option is not supported by Amazon Genomics CLI
and is ignored.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Nextflow</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/nextflow/</link>
      <pubDate>Fri, 01 Oct 2021 17:27:31 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/nextflow/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nextflow.io/&#34;&gt;Nextflow&lt;/a&gt; is free open source software distributed under the Apache 2.0 licence
developed by &lt;a href=&#34;http://www.seqera.io/&#34;&gt;Seqera&lt;/a&gt; Labs.
The project was started in the Notredame Lab at the &lt;a href=&#34;http://www.crg.eu/&#34;&gt;Centre for Genomic Regulation (CRG)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The source code for Nextflow is available on &lt;a href=&#34;https://github.com/nextflow-io/nextflow&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;There are four components of a Nextflow engine as deployed in an Amazon Genomics CLI context:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;NextflowContextArch.png&#34; alt=&#34;Image of infrastructure deployed in a Nextflow context&#34; title=&#34;Nextflow Context Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;wes-adapter&#34;&gt;WES Adapter&lt;/h3&gt;
&lt;p&gt;Amazon Genomics CLI communicates with the Nextflow engine via a GA4GH &lt;a href=&#34;https://github.com/ga4gh/workflow-execution-service-schemas&#34;&gt;WES&lt;/a&gt; REST service. The WES Adapter implements
the WES standard and translates WES calls into calls to the Nextflow head process.&lt;/p&gt;
&lt;h3 id=&#34;head-compute-environment&#34;&gt;Head Compute Environment&lt;/h3&gt;
&lt;p&gt;For every workflow submitted, the WES adapter will create a new AWS Batch Job that contains the Nextflow process responsible
for running that workflow. These Nextflow &amp;ldquo;head&amp;rdquo; jobs are run in an &amp;ldquo;On-demand&amp;rdquo; compute environment even when the actual workflow
tasks run in a Spot environment. This is to prevent Spot interruptions from terminating the workflow coordinator.&lt;/p&gt;
&lt;h3 id=&#34;task-compute-environment&#34;&gt;Task Compute Environment&lt;/h3&gt;
&lt;p&gt;Workflow tasks are submitted by the Nextflow head job to an AWS Batch queue and run in containers using an AWS Compute Environment.
Container characteristics are defined by the resources requested in the workflow configuration. AWS Batch coordinates the elastic provisioning of EC2 instances (container hosts)
based on the available work in the queue. Batch will place containers on container hosts as space allows.&lt;/p&gt;
&lt;h4 id=&#34;fetch-and-run-strategy&#34;&gt;Fetch and Run Strategy&lt;/h4&gt;
&lt;p&gt;Execution of workflow tasks uses a &amp;ldquo;Fetch and Run&amp;rdquo; strategy. Input files required by a workflow task are fetched from
S3 into the task container. Output files are copied out of the container to S3.&lt;/p&gt;
&lt;h4 id=&#34;disk-expansion&#34;&gt;Disk Expansion&lt;/h4&gt;
&lt;p&gt;Container hosts in the Batch compute environment use EBS volumes as local scratch space. As an EBS volume approaches a
capacity threshold, new EBS volumes will be attached and merged into the file system. These volumes are destroyed when
AWS Batch terminates the container host.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Snakemake</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/snakemake/</link>
      <pubDate>Fri, 01 Oct 2021 17:27:31 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/workflow-engines/snakemake/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://snakemake.readthedocs.io/en/stable/&#34;&gt;Snakemake&lt;/a&gt; is free open source software distributed under the MIT licence
developed by &lt;a href=&#34;https://snakemake.readthedocs.io/en/stable/project_info/authors.html&#34;&gt;Johannes KÃ¶ster and their team&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The source code for snakemake is available on &lt;a href=&#34;https://github.com/snakemake/snakemake&#34;&gt;GitHub&lt;/a&gt;. When deployed with
Amazon Genomics CLI snakemake uses Batch to distribute the underlying tasks.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;There are four components of a snakemake engine as deployed in an Amazon Genomics CLI context:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SnakeMakeContextArch.png&#34; alt=&#34;Image of infrastructure deployed in a SnakeMake context&#34; title=&#34;SnakeMake Context Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;wes-adapter&#34;&gt;WES Adapter&lt;/h3&gt;
&lt;p&gt;Amazon Genomics CLI communicates with the snakemake engine via a GA4GH &lt;a href=&#34;https://github.com/ga4gh/workflow-execution-service-schemas&#34;&gt;WES&lt;/a&gt; REST service. The WES Adapter implements
the WES standard and translates WES calls into calls to the snakemake head process.&lt;/p&gt;
&lt;h3 id=&#34;head-compute-environment&#34;&gt;Head Compute Environment&lt;/h3&gt;
&lt;p&gt;For every workflow submitted, the WES adapter will create a new AWS Batch Job that contains the snakemake process responsible
for running that workflow. These snakemake &amp;ldquo;head&amp;rdquo; jobs are run in an &amp;ldquo;On-demand&amp;rdquo; AWS Fargate compute environment even when the actual workflow
tasks run in a Spot environment. This is to prevent Spot interruptions from terminating the workflow coordinator.&lt;/p&gt;
&lt;h3 id=&#34;task-compute-environment&#34;&gt;Task Compute Environment&lt;/h3&gt;
&lt;p&gt;Workflow tasks are submitted by the snakemake head job to an AWS Batch queue and run in containers using an AWS Compute Environment.
Container characteristics are defined by the resources requested in the workflow configuration. AWS Batch coordinates the elastic provisioning of EC2 instances (container hosts)
based on the available work in the queue. Batch will place containers on container hosts as space allows.&lt;/p&gt;
&lt;h4 id=&#34;session-cache-and-input-localization&#34;&gt;Session Cache and Input Localization&lt;/h4&gt;
&lt;p&gt;Any context with a snakemake engine will use an Amazon Elastic File System (EFS) volume as scratch space. Inputs from the workflow
are localized to the volume by jobs that the snakemake engine spawns to copy these files to the volume. Outputs are copied back
to S3 after the workflow is complete. Workflow tasks access the EFS volume to obtain inputs and write intermediates and outputs.&lt;/p&gt;
&lt;p&gt;The EFS volume can used by all snakemake engine &amp;ldquo;head&amp;rdquo; jobs to store metadata necessary for dependency caching by specifying an argument
for the conda workspace that is common across all executions. An example of this is &lt;code&gt;--conda-prefix /mnt/efs/snakemake/conda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The EFS volume will remain in your account for the lifetime of the context and are destroyed when contexts are destroyed.
Because the volume will grow in size as you run more workflows we recommend destroying the context when done to avoid on going EFS
charges.&lt;/p&gt;
&lt;h2 id=&#34;using-snakemake-as-a-context-engine&#34;&gt;Using Snakemake as a Context Engine&lt;/h2&gt;
&lt;p&gt;You may declare snakemake to be the &lt;code&gt;engine&lt;/code&gt; for any contexts &lt;code&gt;snakemake&lt;/code&gt; type engine. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;contexts&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;onDemandCtx&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;requestSpotInstances&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;engines&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;      &lt;/span&gt;- &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;snakemake&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;engine&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;snakemake&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;conda-dependency-caching&#34;&gt;Conda Dependency Caching&lt;/h2&gt;
&lt;p&gt;Dependency caching is disabled by default so that each workflow can be run independently. If you would like workflow
runs to re-use the Conda cache then please specify a folder under &amp;ldquo;/mnt/efs&amp;rdquo; which is where the EFS storage space is
attached. This will enable snakemake to re-use the dependency which will decrease the time that subsequent workflow runs
will take.&lt;/p&gt;
&lt;p&gt;To disable call caching you can provide the &lt;code&gt;--conda-prefix&lt;/code&gt; engine option. You may do this in a workflows &lt;code&gt;MANIFEST.json&lt;/code&gt; by
adding the following key/ value pair.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  &amp;quot;engineOptions&amp;quot;: &amp;quot;-j 10 --conda-prefix /mnt/efs/snakemake/conda&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
  </channel>
</rss>
