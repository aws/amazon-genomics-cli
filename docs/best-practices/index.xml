<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amazon Genomics CLI â€“ Best Practices</title>
    <link>https://aws.github.io/amazon-genomics-cli/docs/best-practices/</link>
    <description>Recent content in Best Practices on Amazon Genomics CLI</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 17 Sep 2021 17:55:46 -0400</lastBuildDate>
    
	  <atom:link href="https://aws.github.io/amazon-genomics-cli/docs/best-practices/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Controlling Costs</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/best-practices/controllingcosts/</link>
      <pubDate>Fri, 17 Sep 2021 18:01:56 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/best-practices/controllingcosts/</guid>
      <description>
        
        
        &lt;p&gt;When you begin to run large scale workflows frequently it will become important to be able to understand the costs involved and
how to optimize your workflow and use of Amazon Genomics CLI to reduce costs.&lt;/p&gt;
&lt;h2 id=&#34;use-aws-cost-explorer-to-report-on-costs&#34;&gt;Use AWS Cost Explorer to Report on Costs&lt;/h2&gt;
&lt;p&gt;AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.
We recommend you use this tool to gain sight into the costs of running your genomics workflows. At the time of writing AWS Cost Explorer
can only be enabled using the AWS Console so Amazon Genomics CLI won&amp;rsquo;t be able to set this up for you. As a first step you will need to &lt;a href=&#34;https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-getting-started.html&#34;&gt;enable cost explorer&lt;/a&gt; for your
AWS account.&lt;/p&gt;
&lt;p&gt;Amazon Genomics CLI will tag the infrastructure it creates with tags. Application, user, project and context tags are all generated as
appropriate and these can be used as &lt;a href=&#34;https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html&#34;&gt;cost allocation tags&lt;/a&gt;
to determine which account costs are coming from Amazon Genomics CLI and which user, context and project.&lt;/p&gt;
&lt;p&gt;Within Cost Explorer the Amazon Genomics CLI tags will be referred to as &lt;a href=&#34;https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html&#34;&gt;&amp;ldquo;User Defined Cost Allocation Tags&amp;rdquo;&lt;/a&gt;.
Before a tag can be used in a cost report it must be &lt;a href=&#34;https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html&#34;&gt;activated&lt;/a&gt;. Costs associated with
tags are only available for infrastructure used &lt;em&gt;after&lt;/em&gt; activation of a tag, so it will not be possible to retrospectively
examine costs.&lt;/p&gt;
&lt;h2 id=&#34;optimizing-requested-container-resources&#34;&gt;Optimizing Requested Container Resources&lt;/h2&gt;
&lt;p&gt;Tasks in a workflow typically run in Docker containers. Depending on the workflow language there will be some kind of &lt;code&gt;runtime&lt;/code&gt; definition that specifies the
number of vCPUs and amount of RAM allocated to the task. For example, in WDL you could specify&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  runtime {
    docker: &amp;quot;biocontainers/plink1.9:v1.90b6.6-181012-1-deb_cv1&amp;quot;
    memory: &amp;quot;8 GB&amp;quot;
    cpu: 2
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The amount of resource allocated to each container ultimately impacts the cost to run a workflow. Optimally allocating
resources leads to cost efficiency.&lt;/p&gt;
&lt;h2 id=&#34;optimize-the-longest-running-and-most-parallel-tasks-first&#34;&gt;Optimize the longest running, and most parallel tasks first&lt;/h2&gt;
&lt;p&gt;When optimizing a workflow, focus on those tasks that run the longest as well as those
that have the largest number of parallel tasks as they will make up the majority of the workflow runtime and contribute
most to the cost.&lt;/p&gt;
&lt;h2 id=&#34;consider-cpu-and-memory-ratios&#34;&gt;Consider CPU and memory ratios&lt;/h2&gt;
&lt;p&gt;EC2 workers for Cromwell AWS Batch compute environments are &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;m&lt;/code&gt;, and &lt;code&gt;r&lt;/code&gt; instance families that
have vCPU to memory ratios of 1:2, 1:4 and 1:8 respectively. Engines that run container based workflows will typically attempt to fit containers to instances in
the most optimal way depending on cost and size requirements, or they will delegate this to a service like AWS Batch. Given that a task requiring 16GB of RAM that could make
use of all available CPUs, then to optimally pack the containers you should specify either 2, 4, or 8 vCPU. Other
values could lead to inefficient packing meaning the resources of the EC2 container instance will be paid for but
not optimally used.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Fully packing an instance can result in it becoming unresponsive if the tasks in the containers use 100%
(or more if they start swapping) of the allocated resources. The instance may then be unresponsive to its management services or the workflow engine and may
time out. To avoid this, always allow for a little overhead, especially in the smaller instances.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The largest instance types deployed by default are from the &lt;code&gt;4xlarge&lt;/code&gt; size which have 16 vCPU and up to 128 MB of RAM.&lt;/p&gt;
&lt;h2 id=&#34;consider-splitting-tasks-that-pipe-output&#34;&gt;Consider splitting tasks that pipe output&lt;/h2&gt;
&lt;p&gt;If a workflow task consists of a process that pipes &lt;code&gt;STDOUT&lt;/code&gt; to another process then both processes will run in the same
container and receive the same resources. If one task requires more resources than the other this might be inefficient,
it may be better divided into two tasks each with its own &lt;code&gt;runtime&lt;/code&gt; configuration. Note that this will require the
intermediate &lt;code&gt;STDOUT&lt;/code&gt; to be written to a file and copied between containers so if this output is very large then keeping
the processes in the same task may be more efficient. Piping very large outputs may a lot of memory so
your container will need an appropriate allocation of memory.&lt;/p&gt;
&lt;h2 id=&#34;use-the-most-cost-effective-instance-generation&#34;&gt;Use the most cost-effective instance generation&lt;/h2&gt;
&lt;p&gt;When you specify the &lt;code&gt;instanceTypes&lt;/code&gt; in a context, as opposed to letting Amazon Genomics CLI do it for you, consider the cost and performance of the instance types with respect to your workflow requirements.
Fifth generation EC2 types (&lt;code&gt;c5&lt;/code&gt;, &lt;code&gt;m5&lt;/code&gt;, &lt;code&gt;r5&lt;/code&gt;) have a lower on-demand price and have higher clock speeds than their 4th
generation counterparts (&lt;code&gt;c4&lt;/code&gt;, &lt;code&gt;m4&lt;/code&gt;, &lt;code&gt;r4&lt;/code&gt;). Therefore, for on-demand compute environments, those instance types should be
preferred. In spot compute environments we suggest using both 4th and 5th generation types as this increases the pool of
available types meaning Batch will be able to choose the instance type that is cheapest and least likely to be
interrupted.&lt;/p&gt;
&lt;h2 id=&#34;deploy-amazon-genomics-cli-where-your-s3-data-is&#34;&gt;Deploy Amazon Genomics CLI where your S3 data is&lt;/h2&gt;
&lt;p&gt;Genomics workflows may need to access considerable amounts of data stored in S3. Although S3 uses global namespaces, buckets
do reside in regions. If you access a lot of S3 data it makes sense to deploy your Amazon Genomics CLI infrastructure in the same region
to avoid cross region data charges.&lt;/p&gt;
&lt;p&gt;Further, if you use a custom VPC we recommend deploying a VPC endpoint for S3 so that you do no incur NAT Gateway charges
for data coming from the same region. If you do not you might find that NAT Gateway charges are the largest part of your
workflow run costs. If you allow Amazon Genomics CLI to create your VPC (the default), appropriate VPC endpoints will be setup for you.
Note that VPC endpoints cannot avoid cross region data charges, so you will still want to deploy in the region where most of
your data resides.&lt;/p&gt;
&lt;h2 id=&#34;use-spot-instances&#34;&gt;Use Spot Instances&lt;/h2&gt;
&lt;p&gt;The use of Spot instances can significantly reduce costs of running workflows. However, spot instances may be interrupted when EC2 demand
is high. Some workflow engines, such as Cromwell, can support retries of tasks that fail due to Spot interruption (among other things).
To enable this for Cromwell, include the &lt;code&gt;awsBatchRetryAttempts&lt;/code&gt; parameter in the &lt;code&gt;runtime&lt;/code&gt; section of a WDL task with an
integer number of attempts.&lt;/p&gt;
&lt;p&gt;Even with retries, there is a risk that spot interruption will case a task or entire workflow to fail. Use of an engines call caching capabilities (if available)
can help avoid repeating work if a partially complete workflow needs to be restarted du to Spot instance interruption.&lt;/p&gt;
&lt;h2 id=&#34;use-private-ecr-registries&#34;&gt;Use private ECR registries&lt;/h2&gt;
&lt;p&gt;Each task in a workflow requires access to a container image, and some of these images can be several GB if they contain
large packages like GATK. This can lead to large NAT Gateway traffic charges. To avoid these charges, we recommend deploying
copies of frequently used container images into your accounts private ECR registry.&lt;/p&gt;
&lt;p&gt;Amazon Genomics CLI deployed VPCs use a VPC gateway to talk to private ECR registries in your account thereby avoiding NAT Gateway traffic. The gateway is
limited to registries in the same region as the VPC, so to avoid cross-region traffic you should deploy images into the region(s) that you
use for Amazon Genomics CLI.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Scaling Workloads</title>
      <link>https://aws.github.io/amazon-genomics-cli/docs/best-practices/scalingworkloads/</link>
      <pubDate>Fri, 17 Sep 2021 17:59:52 -0400</pubDate>
      
      <guid>https://aws.github.io/amazon-genomics-cli/docs/best-practices/scalingworkloads/</guid>
      <description>
        
        
        &lt;p&gt;Workflows with considerable compute requirements can incur large costs and may fail due to infrastructure constraints.
The following considerations will help you design workflows that will perform better at scale.&lt;/p&gt;
&lt;h2 id=&#34;large-compute-requirements&#34;&gt;Large compute requirements&lt;/h2&gt;
&lt;p&gt;By default, contexts created by AGC will allocate compute nodes with a size of up to &lt;code&gt;4xlarge&lt;/code&gt;. These types have 16 vCPU
and up to 128 GB of RAM. If an individual task requires additional resources you may specify these
in the &lt;code&gt;instanceTypes&lt;/code&gt; array of the project context. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;contexts&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;prod&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;requestSpotInstances&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;instanceTypes&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;            &lt;/span&gt;- &lt;span style=&#34;color:#000&#34;&gt;c5.16xlarge&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;            &lt;/span&gt;- &lt;span style=&#34;color:#000&#34;&gt;r5.24xlarge&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;large-data-growth&#34;&gt;Large data growth&lt;/h2&gt;
&lt;p&gt;When using the Nextflow or Cromwell engines the EC2 container instances that carry out the work use a script to detect
and automatically expand disk capacity. Generally, this will allow disk space to increase to the amount required to hold
inputs, scratch and outputs. However, it can take up to a minute to attach new storage so events that fill disk space
in under a minute can result in failure.&lt;/p&gt;
&lt;h2 id=&#34;large-numbers-of-inputs-outputs&#34;&gt;Large numbers of inputs/ outputs&lt;/h2&gt;
&lt;p&gt;Typically, genomics files are large and best stored in S3. However, most applications used in genomics workflows cannot
read directly from S3. Therefore, these inputs must be localized from S3. Compute work will not be able to begin until localization is complete
so &amp;ldquo;divide and conquer&amp;rdquo; strategies are useful in these cases.&lt;/p&gt;
&lt;p&gt;Whenever possible compress inputs (and outputs) appropriately. The CPU overhead of compression will be low compared to the
network overhead of localization and delocalization.&lt;/p&gt;
&lt;p&gt;Localization of large numbers of large files from S3 will put load on the network interface of the worker nodes and the
node may experience transient network failures or S3 throttling. While we have included retry-with-backoff logic for
localization it is not impossible that downloads may occasionally fail. Failures (and retries) will be recorded in
the workflow task logs.&lt;/p&gt;
&lt;h2 id=&#34;parallel-steps&#34;&gt;Parallel Steps&lt;/h2&gt;
&lt;p&gt;Workflows often contain parallel steps where many individual tasks are computed in parallel. Amazon Genomics CLI makes use of elastic compute
clusters to scale to these requirements. Each context will deploy an elastic compute cluster with a minimum of 0 vCPU and a maximum of 256 vCPU. No individual task
may use more than 256 vCPU. Smaller tasks may be run in parallel up to the maximum of 256 vCPU. Once that limit is met, additional
tasks will be queued to run when capacity becomes free.&lt;/p&gt;
&lt;p&gt;Each parallel task is isolated meaning each task will need a local copy of its inputs. When large numbers of parallel tasks,
require the same inputs (for example reference genomes) you may observe contention for network resources and transient S3
failures. While we have included retry with backoff logic we recommend keeping the number of parallel tasks requiring the same inputs below 500. Fewer,
if the tasks inputs are large.&lt;/p&gt;
&lt;p&gt;An extreme example is Joint Genotyping. This type of analysis benefits from processing large numbers of samples at the same.
Further, the user may wish to genotype many intervals concurrently. Finally, the step of merging the variant calls will
import the variants from all intervals. In our experience, a naive implementation calling 100 samples over 100 intervals
is feasible. Also, feasible is calling ~20 samples over 500 intervals. At larger scales it would be worth considering dividing
tasks by chromosome or batching inputs.&lt;/p&gt;
&lt;h2 id=&#34;container-throttling&#34;&gt;Container throttling&lt;/h2&gt;
&lt;p&gt;Some container registries will throttle container access from anonymous accounts. Because each task in a workflow uses
a container large or frequently run workflows may not be able to access their required containers. While compute clusters
deployed by Amazon Genomics CLI are configured to cache containers this is only available on a per-instance basis. Further, due to the
elastic nature of the clusters instances with cached container images are frequently shutdown. All of this will potentially
lead to an excess of requests. To avoid this we recommend using registries that don&amp;rsquo;t impose these limits, or using images
hosted in an ECR registry in your AWS account.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
